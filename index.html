<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>HyperDreamBooth</title>
<link href="./files/style.css" rel="stylesheet">
<script type="text/javascript" src="./files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./files/jquery.js"></script>
</head>

<body>
<div class="content">
  <h1><strong>HyperDreamBooth: HyperNetworks for Fast <br> Personalization of Text-to-Image Models</strong></h1>
  <p id="authors"><span><a href="https://natanielruiz.github.io/"></a></span><a href="https://natanielruiz.github.io/">Nataniel Ruiz</a	> <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a> <a href="https://varunjampani.github.io/">Varun Jampani</a> <a href="http://www.weiwei.one">Wei Wei</a> <a href="https://scholar.google.com/citations?user=u-UDZcsAAAAJ&hl=en">Tingbo Hou</a> <br>
  <a href="https://research.google/people/106214/">Yael Pritch</a> <a href="https://nealwadhwa.com">Neal Wadhwa</a> <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a> <a href="https://kfiraberman.github.io/">Kfir Aberman</a><br>
    <br>
  <span style="font-size: 24px">Google Research
  </span></p>
  <br>
  <img src="./files/teaser_v2.png" class="teaser-gif" style="width:80%;"><br>
  <h3 style="text-align:center"><em>HyperDreamBooth: smaller, faster, better.</em></h3>
  <p>
    Using only a <i>single</i> input image, <i>HyperDreamBooth</i> is able to personalize a text-to-image diffusion model <b>25x</b> faster than DreamBooth, by using <b>(1)</b> a HyperNetwork to generate an initial prediction of a subset of network weights that are then <b>(2)</b> refined using fast finetuning for high fidelity to subject detail. Our method both <i>conserves model integrity and style diversity</i> while closely approximating the subject's essence and details.
  </p>
    <font size="+2">
          <p style="text-align: center;">
            <a href="./files/paper.pdf" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <!-- <a href="files/bibtex.txt" target="_blank">[BibTeX]</a> -->
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity.
    To overcome these challenges, we propose <b>HyperDreamBooth</b> - a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications.
    Our method achieves personalization on faces in roughly 20 seconds, <b>25x</b> faster than DreamBooth and <b>125x</b> faster than Textual Inversion, using as few as <it>one</it> reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is <b>10000x</b> smaller than a normal DreamBooth model.</p>
</div>
<div class="content">
  <h2>Contributions</h2>
  <p> Our work proposes to tackle the problems of <b>size</b> and <b>speed</b> of DreamBooth, while preserving <b>model integrity</b>, <b>editability</b> and <b>subject fidelity</b>. We propose the following contributions:
    <ul>
        <li><i>Lighweight DreamBooth (LiDB)</i> - a personalized text-to-image model, where the customized part is roughly 100KB of size. This is achieved by training a DreamBooth model in a low-dimensional weight-space generated by a random orthogonal incomplete basis inside of a low-rank adaptation weight space.</li><br>
        <li><i>HyperNetwork</i> architecture that leverages the Lightweight DreamBooth configuration and generates the customized part of the weights for a given subject in a text-to-image diffusion model. These provide a strong directional initialization that allows us to further finetune the model in order to achieve strong subject fidelity within a few iteration. Our method is <b>25x</b> faster than DreamBooth while achieving similar performances.</li><br>
        <li>We propose the technique of <i>rank-relaxed finetuning</i>, where the rank of a LoRA DreamBooth model is relaxed during optimization in order to achieve higher subject fidelity, allowing us to initialize the personalized model with an initial approximation using our HyperNetwork, and then approximate the high-level subject details using rank-relaxed finetuning.</li></p>
    </ul>
  <br>
</div>
<div class="content">
  <h2>HyperNetwork</h2>
  <p> Our approach consists of 3 core elements: Lightweight DreamBooth (LiDB), a HyperNetwork that predicts LiDB weight, and rank-relaxed fast fine-tuning.</p>

  <p>
  The core idea behind <b>Lightweight DreamBooth (LiDB)</b> is to further decompose the weight-space of a rank-1 LoRa residuals. Specifically, we do this using a random orthogonal incomplete basis within the rank-1 LoRA weight-space. We illustrate the idea in the figure below. The approach can also be understood as further decomposing the Down (A) and Up (B) matrices of LoRA into two matrices each, where the "Aux" layers are randomly initialized with row-wise orthogonal vectors and are frozen. Surprisingly, we find that with a=100 and b=50, we obtain models that have only 30K trainable variables and are 120 KB in size, with personalization results that are strong and maintain subject fidelity, editability and style diversity.
  </p>
  <br>
  <img class="summary-img" src="./files/lightweight_dreambooth.png" style="width:60%;"> <br>
  <br>
  <p>
  <b>HyperDreamBooth Training and Fast Fine-Tuning.</b> Phase-1: Training a hypernetwork to predict network weights from a face image, such that a text-to-image diffusion network outputs the person's face from the sentence <it>"a [v] face"</it> if the predicted weights are applied to it. We use pre-computed personalized weights for supervision, using an L2 loss, as well as the vanilla diffusion reconstruction loss. Phase-2: Given a face image, our hypernetwork predicts an initial guess for the network weights, which are then fine-tuned using the reconstruction loss to enhance fidelity.  
  </p>
  <img class="summary-img" src="./files/train_and_ft.png" style="width:76%;"> <br>
  <p> <b>HyperNetwork Architecture:</b> Our hypernetwork consists of a Visual Transformer (ViT) encoder that translates face images into latent face features that are then concatenated to latent layer weight features that are initiated by zeros. A Transformer Decoder receives the sequence of the concatenated features and predicts the values of the weight features in an iterative manner by refining the initial weights with delta predictions. The final layer weight deltas that will be added to the diffusion network are obtained by passing the decoder outputs through learnable linear layers. The transformer decoder is a strong fit for this type of weight prediction task, since the output of a diffusion UNet or Text Encoder is sequentially dependent on the weights of the layers, thus in order to personalize a model there is interdependence of the weights from different layers. In previous work, this dependency is not rigorously modeled in the HyperNetwork, whereas with a transformer decoder with a positional embedding, this positional dependency is modeled - similar to dependencies between words in a language model transformer.</p>
  <br>
  <img class="summary-img" src="./files/HeprNetwork_scheme.png" style="width:100%;"> <br>
  <p>HyperNetwork + Fast Finetuning achieves strong results. Below we show, for each reference (row), outputs from the initial hypernetwork prediction (HyperNetwork Prediction column), as well as results after HyperNetwork prediction and fast finetuning (HyperNetwork + Fast Finetuning). We also show generated results without the HyperNetwork prediction component, demonstrating its importance.</p>
  <img class="summary-img" src="./files/intermediate_hypernet.png" style="width:100%;"> <br>

</div>
<div class="content">
  <h2>Results</h2>
  <p><b>Results Gallery:</b> Our method can generate novel artistic and stylized results of diverse subjects (depicted in an input image, left) with considerable editability while maintaining the integrity to the subject's key facial characteristics. The output images were generated with the following captions (top-left to bottom-right): <i>"An Instagram selfie of a [V] face"</i>, <i>"A Pixar character of a [V] face"</i>, <i>"A [V] face with bark skin"</i>, <i>A [V] face as a rock star</i>}. Rightmost: <i>A professional shot of a [V] face"</i>.</p>
<img class="summary-img" src="./files/results_gallery.png" style="width:80%;">
</div>
<div class="content">
  <h2>Comparisons</h2>
  <p>We compare random generated samples for our method (HyperDreamBooth), DreamBooth and Textual Inversion for two different identities and five different stylistic prompts. We observe that our method generally achieves very strong editability while preserving identity, generally surpassing competing methods in the single-reference regime. For quantitative comparisons, including a user study please refer to the paper.</p>
  <br>
  <img class="summary-img" src="./files/comparison.png" style="width:100%;"> <br>
</div>
<!-- <div class="content">
  <h2>User Study and Metrics</h2>
  <p>User study and metrics</p>
  <br>
  <img class="summary-img" src="./files/novel_views.png" style="width:100%;"> <br>
</div> -->
<div class="content">
  <h2>Societal Impact</h2>
  <p>This work aims to empower users with a tool for augmenting their creativity and ability to express themselves through creations in an intuitive manner. However, advanced methods for image generation can affect society in complex ways. Our proposed method inherits many possible concerns that affect this class of image generation, including altering sensitive personal characteristics such as skin color, age and gender, as well as reproducing unfair bias that can already be found in pre-trained model's training data. The underlying open source pre-trained model used in our work, Stable Diffusion, exhibits some of these concerns. All concerns related to our work have been present in the litany of recent personalization work, and the only augmented risk is that our method is more efficient and faster than previous work. In particular, we haven't found in our experiments any difference with respect to previous work on bias, or harmful content, and we have qualitatively found that our method works equally well across different ethnicities, ages, and other important personal characteristics. Nevertheless, future research in generative modeling and model personalization must continue investigating and revalidating these concerns.</p>
  <br>
</div>
<!-- <div class="content">
  <h2>BibTex</h2>
  <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code> 
</div> -->
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Kihyuk Sohn, Kathy Meier-Hellstern and Luming Tang for their insightful feedback that helped improve this work. We also thank Moab Arar, Rinon Gal and Daniel Cohen-Or for interesting conversations and support. And finally, a particular and special thank you to Jason Baldridge for his feedback and support for this project.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). 
  </p> -->
</div>
</body>
</html>
